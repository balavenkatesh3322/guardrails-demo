#CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose
huggingface_hub 
streamlit 
langchain
pydantic
accelerate
torch==1.13
#faiss-gpu
#sentence-transformers